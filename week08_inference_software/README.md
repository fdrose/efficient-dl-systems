# Week 8: LLM inference optimizations and software

* Lecture: [link](./lecture.pdf)
* Seminar: [link](./seminar/seminar.ipynb)

## Further reading
* [What is the KV cache?](https://mett29.github.io/posts/kv-cache/)
* [Overview of torch.compiler](https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler-overview)
* [Torch Dynamo Overview](https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html)
* [Torch Dynamo Deep-Dive](https://pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html)
* [Torch Compiler Troubleshouting](https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst)
* [Deep Dive into Triton Internals (3 Parts)](https://www.kapilsharma.dev/posts/deep-dive-into-triton-internals/)
* [HF Ultra-Scale Playbook (fused kernels section)](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=fused_kernels)
* [Liger Kernels repo](https://github.com/linkedin/Liger-Kernel/tree/main/src/liger_kernel/ops)
* [Liger Kernels paper](https://arxiv.org/pdf/2410.10989)
* [FlashAttention](https://arxiv.org/pdf/2205.14135)
* [FlassAttention2](https://arxiv.org/pdf/2307.08691)
* [FlassAttention3](https://arxiv.org/pdf/2407.08608)
* [Flex Attention Tutorial](https://pytorch.org/blog/flexattention/)
