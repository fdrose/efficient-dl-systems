{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 6: torch.compile, kernel fusion, and parallelisms\n",
        "\n",
        "### Seminar outline\n",
        "1. Kernel Fusion\n",
        "    - Why fusing operations matters\n",
        "    - GPT-OSS SwiGLU fusion example\n",
        "2. Internals of torch.compile\n",
        "    - Basic torch.compile example\n",
        "    - Understanding What Dynamo Captures\n",
        "    - Graph Breaks\n",
        "    - Extra. The 3 stages of torch.compile.\n",
        "        - TorchDynamo: bytecode capture and FX graphs\n",
        "        - AOTAutograd: forward and backward tracing\n",
        "        - TorchInductor: Triton/C++ kernel generation\n",
        "3. GPU Memory Hierarchy\n",
        "    - Cache Behavior in Matmul\n",
        "    - ...\n",
        "4. Efficient Cross Entropy\n",
        "    - Liger Kernel Cross Entropy\n",
        "    - Fused Linear Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA H100 80GB HBM3\n",
            "PyTorch version: 2.10.0+cu128\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Kernel Fusion\n",
        "\n",
        "### Why Kernel Fusion Matters\n",
        "\n",
        "**Problem:**\n",
        "- GPU compute is much faster than memory\n",
        "- H100: 800 TFLOPS (25 TFLOPS for CUDA Cores) compute vs 2.4 TB/s memory bandwidth\n",
        "- For elementwise ops, we're almost always memory-bound\n",
        "\n",
        "**Why Fusion Helps:**\n",
        "- Reduces memory traffic:\n",
        "    - Unfused ops read/write HBM for each operation\n",
        "    - Fused ops keep intermediates in registers\n",
        "- Reduces kernel launch overhead:\n",
        "    - Each CUDA kernel launch has ~5-10Î¼s overhead\n",
        "    - Fusing N ops into 1 kernel eliminates N-1 launches\n",
        "\n",
        "<p float=\"left\">\n",
        "<img src=\"./images/fused_kernels1.png\" width=\"400\"/>\n",
        "<img src=\"./images/fused_kernels2.png\" width=\"400\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT-OSS SwiGLU fusion\n",
        "\n",
        "SwiGLU is used in modern LLMs:\n",
        "\n",
        "$$\\text{SwiGLU}(x, y) = x \\odot \\text{SiLU}(y), \\quad \\text{SiLU}(x) = x \\cdot \\sigma(x)$$\n",
        "\n",
        "The elementwise multiplication and SiLU are perfect fusion candidates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "from liger_kernel.ops.utils import calculate_settings\n",
        "\n",
        "\n",
        "def swiglu_unfused(gate, up):\n",
        "    \"\"\"Each operation launches a separate CUDA kernel\"\"\"\n",
        "    return F.silu(gate) * up\n",
        "\n",
        "\n",
        "@torch.compile\n",
        "def swiglu_compiled(gate, up):\n",
        "    \"\"\"torch.compile fuses silu + mul into a single kernel\"\"\"\n",
        "    return F.silu(gate) * up\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _silu(x):\n",
        "    return x * tl.sigmoid(x)\n",
        "\n",
        "@triton.jit\n",
        "def _swiglu_forward_kernel(a_ptr, b_ptr, c_ptr, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n",
        "    program_id = tl.program_id(0).to(tl.int64)\n",
        "\n",
        "    # locate start index\n",
        "    a_ptr += program_id * stride\n",
        "    b_ptr += program_id * stride\n",
        "    c_ptr += program_id * stride\n",
        "\n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "    mask = col_offsets < n_cols\n",
        "\n",
        "    # sigmoid requires type float32\n",
        "    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)\n",
        "    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n",
        "    c_row = _silu(a_row) * b_row\n",
        "    tl.store(c_ptr + col_offsets, c_row, mask=mask)\n",
        "\n",
        "\n",
        "def swiglu_forward(a, b):\n",
        "    ori_shape = a.shape\n",
        "\n",
        "    n_cols = ori_shape[-1]\n",
        "    a = a.view(-1, n_cols)\n",
        "    b = b.view(-1, n_cols)\n",
        "    c = torch.empty_like(a)\n",
        "    n_rows = a.shape[0]\n",
        "\n",
        "    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n",
        "\n",
        "    _swiglu_forward_kernel[(n_rows,)](\n",
        "        a,\n",
        "        b,\n",
        "        c,\n",
        "        c.stride(-2),\n",
        "        n_cols=n_cols,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "        num_warps=num_warps,\n",
        "    )\n",
        "    return a, b, c.view(*ori_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unfused:       0.117 ms\n",
            "torch.compile: 0.069 ms\n",
            "Triton:        0.069 ms\n"
          ]
        }
      ],
      "source": [
        "def benchmark(fn, gate, up, warmup=10, iters=1000):\n",
        "    for _ in range(warmup):\n",
        "        _ = fn(gate, up)\n",
        "\n",
        "    start_event = torch.cuda.Event(enable_timing=True)\n",
        "    end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start_event.record()\n",
        "\n",
        "    for _ in range(iters):\n",
        "        _ = fn(gate, up)\n",
        "\n",
        "    end_event.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    return start_event.elapsed_time(end_event) / iters\n",
        "\n",
        "\n",
        "gate = torch.randn(4096, 4096, device=device)\n",
        "up = torch.randn(4096, 4096, device=device)\n",
        "\n",
        "for _ in range(10):\n",
        "    _ = swiglu_unfused(gate, up)\n",
        "    _ = swiglu_compiled(gate, up)\n",
        "    _ = swiglu_forward(gate, up)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "print(f\"Unfused:       {benchmark(swiglu_unfused, gate, up):.3f} ms\")\n",
        "print(f\"torch.compile: {benchmark(swiglu_compiled, gate, up):.3f} ms\")\n",
        "print(f\"Triton:        {benchmark(swiglu_forward, gate, up):.3f} ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**OpenAI gpt-oss SwiGLU variant:**\n",
        "\n",
        "$$\\text{SwiGLU}_{\\text{gpt-oss}}(x, y) = x \\cdot \\sigma(\\alpha x) \\cdot (y + 1)$$\n",
        "\n",
        "Using the identity $\\sigma(z) = \\frac{1}{2}(1 + \\tanh(\\frac{z}{2}))$, this becomes:\n",
        "\n",
        "$$\\frac{x}{2} \\cdot \\left(1 + \\tanh\\left(\\frac{\\alpha x}{2}\\right)\\right) \\cdot (y + 1)$$\n",
        "\n",
        "**Optimized form (5 ops: FMUL, FMUL, TANH, FFMA, FFMA):**\n",
        "\n",
        "$$\\text{Let } h = \\frac{x}{2}, \\quad s = h \\cdot \\tanh(\\alpha h) + h$$\n",
        "$$\\text{SwiGLU}_{\\text{gpt-oss}}(x, y) = s \\cdot y + s$$\n",
        "\n",
        "```\n",
        "h = x * 0.5           // FMUL #1\n",
        "t = alpha * h         // FMUL #2  \n",
        "t = tanh(t)           // TANH\n",
        "s = h * t + h         // FFMA #1  (fused multiply-add)\n",
        "out = s * y + s       // FFMA #2  (fused multiply-add)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "ALPHA = 1.0\n",
        "\n",
        "def swiglu_gptoss_unfused(x, y):\n",
        "    \"\"\"GPT-OSS style: x * sigmoid(a*x) * (y + 1) - unfused\"\"\"\n",
        "    return x * torch.sigmoid(ALPHA * x) * (y + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] Output code: \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # AOT ID: ['39_inference']\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import torch\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import math\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import random\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import os\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import tempfile\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from math import inf, nan\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from cmath import nanj\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.utils import maybe_profile\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch import device, empty_strided\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import triton\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import triton.language as tl\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] aten = torch.ops.aten\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] inductor_ops = torch.ops.inductor\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] _quantized = torch.ops._quantized\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] async_compile = AsyncCompile()\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # kernel path: /tmp/torchinductor_mabraham/x2/cx2e5nfmlof2k56awkj7uzttw5lwretp3rmtnhqhqpo7mqo2mpmt.py\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # Topologically Sorted Source Nodes: [mul, sigmoid, mul_1, add, mul_2], Original ATen: [aten.mul, aten.sigmoid, aten.add]\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # Source node to ATen node mapping:\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   add => add\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   mul => mul\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   mul_1 => mul_1\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   mul_2 => mul_2\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   sigmoid => sigmoid\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # Graph fragment:\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %arg0_1 : Tensor \"f32[4096, 4096][4096, 1]cuda:0\" = PlaceHolder[target=arg0_1]\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %arg1_1 : Tensor \"f32[4096, 4096][4096, 1]cuda:0\" = PlaceHolder[target=arg1_1]\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %mul : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, 1.0), kwargs = {})\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %sigmoid : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%mul,), kwargs = {})\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %mul_1 : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %sigmoid), kwargs = {})\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %add : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg1_1, 1), kwargs = {})\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %mul_2 : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1, %add), kwargs = {})\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   return %mul_2\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] triton_poi_fused_add_mul_sigmoid_0 = async_compile.triton('triton_poi_fused_add_mul_sigmoid_0', '''\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import triton\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import triton.language as tl\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] @triton_heuristics.pointwise(\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     size_hints={'x': 16777216}, \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     filename=__file__,\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, max_threads_per_block=1024, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_sigmoid_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': '91D1D1166449D9E2EB66EB9770C9E2C3B0FEB787341F27FEB4D3F0338DA50336', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 268435456}},\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     min_elem_per_thread=0\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] )\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] @triton.jit\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] def triton_poi_fused_add_mul_sigmoid_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     xnumel = 16777216\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     x0 = xindex\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), None)\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp5 = tl.load(in_ptr1 + (x0), None)\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp1 = 1.0\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp2 = tmp0 * tmp1\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp3 = tl.sigmoid(tmp2)\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp4 = tmp0 * tmp3\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp6 = tmp5 + tmp1\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp7 = tmp4 * tmp6\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp7, None)\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] ''', device_str='cuda')\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] async_compile.wait(globals())\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] del async_compile\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] class Runner:\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     def __init__(self, partitions):\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         self.partitions = partitions\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     def recursively_apply_fns(self, fns):\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         new_callables = []\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             new_callables.append(fn(c))\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         self.partitions = new_callables\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     def call(self, args):\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         arg0_1, arg1_1 = args\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         args.clear()\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         assert_size_stride(arg0_1, (4096, 4096), (4096, 1))\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         assert_size_stride(arg1_1, (4096, 4096), (4096, 1))\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             torch.cuda.set_device(0)\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             buf0 = empty_strided_cuda((4096, 4096), (4096, 1), torch.float32)\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             # Topologically Sorted Source Nodes: [mul, sigmoid, mul_1, add, mul_2], Original ATen: [aten.mul, aten.sigmoid, aten.add]\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             stream0 = get_raw_stream(0)\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             triton_poi_fused_add_mul_sigmoid_0.run(arg0_1, arg1_1, buf0, 16777216, stream=stream0)\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             del arg0_1\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             del arg1_1\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         return (buf0, )\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] runner = Runner(partitions=[])\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] call = runner.call\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     from torch._inductor.utils import print_performance\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     arg0_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     arg1_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1])\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] if __name__ == \"__main__\":\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\n",
            "V0217 10:17:49.995000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:49.998000 1912855 torch/_inductor/codecache.py:1251] [0/0] [__output_code] Output code written to: /tmp/torchinductor_mabraham/ol/coloovikkfiji34xfkydniat4aziwe5iczwhtbn2jnxbqd3grcfj.py\n"
          ]
        }
      ],
      "source": [
        "torch._dynamo.reset()\n",
        "torch._logging.set_logs(output_code=True)\n",
        "\n",
        "@torch.compile\n",
        "def swiglu_gptoss_inspect(x, y):\n",
        "    return x * torch.sigmoid(ALPHA * x) * (y + 1)\n",
        "\n",
        "x = torch.randn(4096, 4096, device=device)\n",
        "y = torch.randn(4096, 4096, device=device)\n",
        "_ = swiglu_gptoss_inspect(x, y)\n",
        "\n",
        "torch._logging.set_logs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] Output code: \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # AOT ID: ['40_inference']\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import torch\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import math\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import random\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import os\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import tempfile\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from math import inf, nan\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from cmath import nanj\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.utils import maybe_profile\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch import device, empty_strided\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import triton\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import triton.language as tl\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] aten = torch.ops.aten\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] inductor_ops = torch.ops.inductor\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] _quantized = torch.ops._quantized\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] async_compile = AsyncCompile()\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # kernel path: /tmp/torchinductor_mabraham/3z/c3zeyqd5itpfjdc3spnu5j77u2zdewu4bbbxraz6mw6wdex3emqm.py\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # Topologically Sorted Source Nodes: [x_half, mul_1, tanh, mul_2, silu_x, mul_3, add_1], Original ATen: [aten.mul, aten.tanh, aten.add]\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # Source node to ATen node mapping:\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   add_1 => add_1\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   mul_1 => mul_1\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   mul_2 => mul_2\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   mul_3 => mul_3\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   silu_x => add\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   tanh => tanh\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   x_half => mul\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # Graph fragment:\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %arg0_1 : Tensor \"f32[4096, 4096][4096, 1]cuda:0\" = PlaceHolder[target=arg0_1]\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %arg1_1 : Tensor \"f32[4096, 4096][4096, 1]cuda:0\" = PlaceHolder[target=arg1_1]\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %mul : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=3] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, 0.5), kwargs = {})\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %mul_1 : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul, 1.0), kwargs = {})\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %tanh : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.tanh.default](args = (%mul_1,), kwargs = {})\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %mul_2 : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul, %tanh), kwargs = {})\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %add : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_2, %mul), kwargs = {})\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %mul_3 : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add, %arg1_1), kwargs = {})\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %add_1 : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_3, %add), kwargs = {})\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   return %add_1\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] triton_poi_fused_add_mul_tanh_0 = async_compile.triton('triton_poi_fused_add_mul_tanh_0', '''\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import triton\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import triton.language as tl\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] @triton_heuristics.pointwise(\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     size_hints={'x': 16777216}, \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     filename=__file__,\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, max_threads_per_block=1024, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_tanh_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': '91D1D1166449D9E2EB66EB9770C9E2C3B0FEB787341F27FEB4D3F0338DA50336', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 268435456}},\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     min_elem_per_thread=0\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] )\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] @triton.jit\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] def triton_poi_fused_add_mul_tanh_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     xnumel = 16777216\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     x0 = xindex\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), None)\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp8 = tl.load(in_ptr1 + (x0), None)\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp1 = 0.5\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp2 = tmp0 * tmp1\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp3 = 1.0\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp4 = tmp2 * tmp3\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp5 = libdevice.tanh(tmp4)\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp6 = tmp2 * tmp5\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp7 = tmp6 + tmp2\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp9 = tmp7 * tmp8\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp10 = tmp9 + tmp7\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp10, None)\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] ''', device_str='cuda')\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] async_compile.wait(globals())\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] del async_compile\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] class Runner:\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     def __init__(self, partitions):\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         self.partitions = partitions\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     def recursively_apply_fns(self, fns):\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         new_callables = []\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             new_callables.append(fn(c))\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         self.partitions = new_callables\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     def call(self, args):\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         arg0_1, arg1_1 = args\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         args.clear()\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         assert_size_stride(arg0_1, (4096, 4096), (4096, 1))\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         assert_size_stride(arg1_1, (4096, 4096), (4096, 1))\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             torch.cuda.set_device(0)\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             buf0 = empty_strided_cuda((4096, 4096), (4096, 1), torch.float32)\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             # Topologically Sorted Source Nodes: [x_half, mul_1, tanh, mul_2, silu_x, mul_3, add_1], Original ATen: [aten.mul, aten.tanh, aten.add]\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             stream0 = get_raw_stream(0)\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             triton_poi_fused_add_mul_tanh_0.run(arg0_1, arg1_1, buf0, 16777216, stream=stream0)\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             del arg0_1\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             del arg1_1\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         return (buf0, )\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] runner = Runner(partitions=[])\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] call = runner.call\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     from torch._inductor.utils import print_performance\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     arg0_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     arg1_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1])\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] if __name__ == \"__main__\":\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\n",
            "V0217 10:17:50.537000 1912855 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0217 10:17:50.539000 1912855 torch/_inductor/codecache.py:1251] [0/0] [__output_code] Output code written to: /tmp/torchinductor_mabraham/gc/cgc3mq772byl6xpnjtwfq7rqy4m5banmn75ekcv3xfx6yyquggpz.py\n"
          ]
        }
      ],
      "source": [
        "torch._dynamo.reset()\n",
        "torch._logging.set_logs(output_code=True)\n",
        "\n",
        "@torch.compile\n",
        "def swiglu_gptoss_tanh_compiled(x, y):\n",
        "    x_half = 0.5 * x\n",
        "    silu_x = x_half * torch.tanh(ALPHA * x_half) + x_half\n",
        "    return silu_x * y + silu_x\n",
        "\n",
        "x = torch.randn(4096, 4096, device=device)\n",
        "y = torch.randn(4096, 4096, device=device)\n",
        "_ = swiglu_gptoss_tanh_compiled(x, y)\n",
        "\n",
        "torch._logging.set_logs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unfused (eager):     0.282 ms\n",
            "Compiled (sigmoid):  0.069 ms\n",
            "Compiled (tanh):     0.068 ms\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(4096, 4096, device=device)\n",
        "y = torch.randn(4096, 4096, device=device)\n",
        "\n",
        "print(f\"Unfused (eager):     {benchmark(swiglu_gptoss_unfused, x, y):.3f} ms\")\n",
        "print(f\"Compiled (sigmoid):  {benchmark(swiglu_gptoss_inspect, x, y):.3f} ms\")\n",
        "print(f\"Compiled (tanh):     {benchmark(swiglu_gptoss_tanh_compiled, x, y):.3f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Memory Traffic Analysis\n",
        "\n",
        "For inputs `gate` and `up` of shape `(batch, seq, hidden_dim)`:\n",
        "\n",
        "**Unfused SwiGLU:**\n",
        "\n",
        "| Step | Operation | Memory Access |\n",
        "|------|-----------|---------------|\n",
        "| 1 | sigmoid(gate) | read, write |\n",
        "| 2 | gate * sigmoid | 2x read, write |\n",
        "| 3 | silu * up | 2x read, write |\n",
        "| **Total** | **3 kernels** | **8 * (batch, seq, hidden_dim) reads/writes** |\n",
        "\n",
        "**Fused SwiGLU:**\n",
        "\n",
        "| Step | Operation | Memory Access |\n",
        "|------|-----------|---------------|\n",
        "| 1 | silu(gate) * up | 2x read, write |\n",
        "| **Total** | **1 kernel** | **3 * (batch, seq, hidden_dim) reads/writes** |\n",
        "\n",
        "All operations (sigmoid, multiply, multiply) happen in registers - no intermediate writes to HBM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "**Unfused:**\n",
            "  Memory: 5 reads + 3 writes = 8x tensor\n",
            "  Memory time:  0.1603 ms\n",
            "  Compute time: 0.0081 ms\n",
            "  Bound by: MEMORY (19.9x ratio)\n",
            "\n",
            "**Fused:**\n",
            "  Memory: 2 reads + 1 writes = 3x tensor\n",
            "  Memory time:  0.0601 ms\n",
            "  Compute time: 0.0081 ms\n",
            "  Bound by: MEMORY (7.5x ratio)\n"
          ]
        }
      ],
      "source": [
        "# H100 SXM specs\n",
        "MEMORY_BANDWIDTH_TB_S = 3.35\n",
        "CUDA_CORE_TFLOPS = 25\n",
        "\n",
        "def calculate_theoretical_time(\n",
        "    shape, \n",
        "    n_reads, \n",
        "    n_writes, \n",
        "    flops_per_element,\n",
        "    dtype_bytes=4\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate theoretical execution time based on memory and compute.\n",
        "    \n",
        "    Args:\n",
        "        shape: tensor shape\n",
        "        n_reads: number of tensor reads\n",
        "        n_writes: number of tensor writes  \n",
        "        flops_per_element: FLOPs per element (e.g., mul=1, add=1, sigmoidâ10)\n",
        "        dtype_bytes: bytes per element (FP32=4, bf16=2)\n",
        "    \n",
        "    Returns:\n",
        "        memory_time_ms, compute_time_ms, is_memory_bound\n",
        "    \"\"\"\n",
        "    n_elements = 1\n",
        "    for dim in shape:\n",
        "        n_elements *= dim\n",
        "\n",
        "    total_bytes = (n_reads + n_writes) * n_elements * dtype_bytes\n",
        "    total_gb = total_bytes / 1e9\n",
        "    memory_time_ms = total_gb / (MEMORY_BANDWIDTH_TB_S * 1000) * 1000  # TB/s â GB/ms\n",
        "\n",
        "    total_flops = n_elements * flops_per_element\n",
        "    total_tflops = total_flops / 1e12\n",
        "    compute_time_ms = total_tflops / CUDA_CORE_TFLOPS * 1000\n",
        "\n",
        "    is_memory_bound = memory_time_ms > compute_time_ms\n",
        "    return memory_time_ms, compute_time_ms, is_memory_bound\n",
        "\n",
        "\n",
        "shape = (4096, 4096)\n",
        "\n",
        "# Unfused: 3 separate kernels\n",
        "# Kernel 1: sigmoid(gate) - read 1, write 1, ~10 FLOPs (exp, div, etc.)\n",
        "# Kernel 2: gate * sigmoid - read 2, write 1, 1 FLOP\n",
        "# Kernel 3: silu * up - read 2, write 1, 1 FLOP\n",
        "unfused_reads = 1 + 2 + 2  # = 5\n",
        "unfused_writes = 1 + 1 + 1  # = 3\n",
        "unfused_flops = 10 + 1 + 1  # = 12\n",
        "\n",
        "mem_t, comp_t, is_mem = calculate_theoretical_time(shape, unfused_reads, unfused_writes, unfused_flops)\n",
        "print(f\"\\n**Unfused:**\")\n",
        "print(f\"  Memory: {unfused_reads} reads + {unfused_writes} writes = {unfused_reads + unfused_writes}x tensor\")\n",
        "print(f\"  Memory time:  {mem_t:.4f} ms\")\n",
        "print(f\"  Compute time: {comp_t:.4f} ms\")\n",
        "print(f\"  Bound by: {'MEMORY' if is_mem else 'COMPUTE'} ({mem_t/comp_t:.1f}x ratio)\")\n",
        "\n",
        "fused_reads = 2\n",
        "fused_writes = 1\n",
        "fused_flops = 12\n",
        "\n",
        "mem_t, comp_t, is_mem = calculate_theoretical_time(shape, fused_reads, fused_writes, fused_flops)\n",
        "print(f\"\\n**Fused:**\")\n",
        "print(f\"  Memory: {fused_reads} reads + {fused_writes} writes = {fused_reads + fused_writes}x tensor\")\n",
        "print(f\"  Memory time:  {mem_t:.4f} ms\")\n",
        "print(f\"  Compute time: {comp_t:.4f} ms\")\n",
        "print(f\"  Bound by: {'MEMORY' if is_mem else 'COMPUTE'} ({mem_t/comp_t:.1f}x ratio)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Internals of torch.compile\n",
        "\n",
        "The [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html) allows you to compile your existing PyTorch code into optimized kernels. It often achieves significant speedups with just a single line change: wrapping your model or function with `torch.compile()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic torch.compile example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "def simple_fn(x):\n",
        "    x = x * 2\n",
        "    x = x + 1\n",
        "    x = torch.relu(x)\n",
        "    x = x * 0.5\n",
        "    return x\n",
        "\n",
        "compiled_fn = torch.compile(simple_fn)\n",
        "\n",
        "x = torch.randn(1000, 1000, device=device)\n",
        "\n",
        "out1 = simple_fn(x)\n",
        "out2 = compiled_fn(x)  # First call triggers compilation\n",
        "\n",
        "print(f\"Results match: {torch.allclose(out1, out2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding What Dynamo Captures\n",
        "\n",
        "Use `torch._dynamo.explain()` to see what Dynamo captures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph Count: 1\n",
            "Graph Break Count: 0\n",
            "Op Count: 4\n",
            "Break Reasons:\n",
            "Ops per Graph:\n",
            "  Ops 1:\n",
            "    <built-in function mul>\n",
            "    <built-in function add>\n",
            "    <built-in method relu of type object at 0x7f8fc3ce4b40>\n",
            "    <built-in function mul>\n",
            "Out Guards:\n",
            "  Guard 1:\n",
            "    Name: ''\n",
            "    Source: shape_env\n",
            "    Create Function: SHAPE_ENV\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 2:\n",
            "    Name: ''\n",
            "    Source: global\n",
            "    Create Function: DETERMINISTIC_ALGORITHMS\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 3:\n",
            "    Name: ''\n",
            "    Source: global\n",
            "    Create Function: GRAD_MODE\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 4:\n",
            "    Name: ''\n",
            "    Source: global\n",
            "    Create Function: DEFAULT_DEVICE\n",
            "    Guard Types: ['DEFAULT_DEVICE']\n",
            "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 5:\n",
            "    Name: ''\n",
            "    Source: global\n",
            "    Create Function: GLOBAL_STATE\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 6:\n",
            "    Name: ''\n",
            "    Source: global\n",
            "    Create Function: TORCH_FUNCTION_STATE\n",
            "    Guard Types: None\n",
            "    Code List: None\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 7:\n",
            "    Name: ''\n",
            "    Source: global\n",
            "    Create Function: AUTOGRAD_SAVED_TENSORS_HOOKS\n",
            "    Guard Types: ['AUTOGRAD_SAVED_TENSORS_HOOKS']\n",
            "    Code List: ['torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None']\n",
            "    Object Weakref: None\n",
            "    Guarded Class Weakref: None\n",
            "  Guard 8:\n",
            "    Name: \"L['x']\"\n",
            "    Source: local\n",
            "    Create Function: TENSOR_MATCH\n",
            "    Guard Types: ['TENSOR_MATCH']\n",
            "    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\n",
            "    Object Weakref: <weakref at 0x7f8ca85981d0; to 'Tensor' at 0x7f8ca8598d20>\n",
            "    Guarded Class Weakref: <weakref at 0x7f8fe6745940; to 'torch._C._TensorMeta' at 0x21e493a0 (Tensor)>\n",
            "  Guard 9:\n",
            "    Name: \"G['torch']\"\n",
            "    Source: global\n",
            "    Create Function: MODULE_MATCH\n",
            "    Guard Types: ['ID_MATCH']\n",
            "    Code List: [\"___check_obj_id(G['torch'], 140259472467952), type=<module 'torch' from '/home/mabraham/Untied-Ulysses/week06/lib/python3.12/site-packages/torch/__init__.py'>\"]\n",
            "    Object Weakref: <weakref at 0x7f8ca8350f40; to 'module' at 0x7f90b40867f0>\n",
            "    Guarded Class Weakref: <weakref at 0x7f90b8013010; to 'type' at 0x14764b0 (module)>\n",
            "  Guard 10:\n",
            "    Name: \"G['torch'].relu\"\n",
            "    Source: global\n",
            "    Create Function: BUILTIN_MATCH\n",
            "    Guard Types: ['ID_MATCH']\n",
            "    Code List: [\"___check_obj_id(G['torch'].relu, 140259345569872), type=<built-in method relu of type object at 0x7f8fc3ce4b40>\"]\n",
            "    Object Weakref: <weakref at 0x7f8ca83526b0; to 'builtin_function_or_method' at 0x7f90ac781850>\n",
            "    Guarded Class Weakref: <weakref at 0x7f90b7fe04a0; to 'type' at 0x14761b0 (builtin_function_or_method)>\n",
            "Compile Times: TorchDynamo compilation metrics:\n",
            "Function, Runtimes (s)\n",
            "_compile.compile_inner, 0.0498\n",
            "compile_attempt_0, 0.0240\n",
            "bytecode_tracing, 0.0110\n",
            "OutputGraph.call_user_compiler, 0.0004\n",
            "build_guards, 0.0236\n",
            "gc, 0.0002\n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "explanation = torch._dynamo.explain(simple_fn)\n",
        "x = torch.randn(1000, 1000, device=device)\n",
        "\n",
        "result = explanation(x)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Graph Count: 1** - The entire function was captured in a single graph (good!)\n",
        "- **Graph Break Count: 0** - No graph breaks occurred (good!)\n",
        "- **Op Count: 4** - Four operations captured: `mul`, `add`, `relu`, `mul`\n",
        "- **Guards** - Conditions that must remain true for the cached compilation to be reused (e.g., tensor shapes, dtypes, device). If guards fail, Dynamo recompiles.\n",
        "\n",
        "| Guard | What it checks |\n",
        "|-------|----------------|\n",
        "| `SHAPE_ENV` | Symbolic shape constraints are satisfied |\n",
        "| `DETERMINISTIC_ALGORITHMS` | `torch.use_deterministic_algorithms()` unchanged |\n",
        "| `GRAD_MODE` | `torch.is_grad_enabled()` unchanged |\n",
        "| `DEFAULT_DEVICE` | Default device hasn't changed |\n",
        "| `GLOBAL_STATE` | Global PyTorch state unchanged |\n",
        "| `TORCH_FUNCTION_STATE` | `__torch_function__` dispatch unchanged |\n",
        "| `TENSOR_MATCH` | Input tensor properties match (shape, dtype, device, strides) |\n",
        "| `MODULE_MATCH` | The `torch` module is the same object |\n",
        "| `BUILTIN_MATCH` | `torch.relu` is the same builtin function |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graph Breaks\n",
        "\n",
        "**Graph breaks** occur when Dynamo encounters code it can't capture into the graph. Common causes:\n",
        "- **Data-dependent control flow** - value depends on tensor data\n",
        "- **Unsupported operations** - certain Python built-ins or dynamic features\n",
        "- **Non-compilable function calls** - functions Dynamo can't trace into\n",
        "\n",
        "When a graph break happens, Dynamo splits execution: compiled code runs up to the break, then Python takes over, then compilation may resume after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "[GraphCompileReason(reason='generic_jump TensorVariable()', user_stack=[<FrameSummary file /tmp/ipykernel_1912855/2551259946.py, line 5 in fn_with_break>], graph_break=True)]\n"
          ]
        }
      ],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "def fn_with_break(x):\n",
        "    x = x * 2\n",
        "    if x.sum() > 0:\n",
        "        x = x + 1\n",
        "    else:\n",
        "        x = x - 1\n",
        "    return x\n",
        "\n",
        "explanation = torch._dynamo.explain(fn_with_break)\n",
        "x = torch.randn(10, 10, device=device)\n",
        "result = explanation(x)\n",
        "print(result.graph_break_count)\n",
        "print(result.break_reasons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "def fn_without_break(x):\n",
        "    x = x * 2\n",
        "    if x.shape[0] > 5:\n",
        "        x = x + 1\n",
        "    else:\n",
        "        x = x - 1\n",
        "    return x\n",
        "\n",
        "explanation = torch._dynamo.explain(fn_without_break)\n",
        "x = torch.randn(10, 10, device=device)\n",
        "result = explanation(x)\n",
        "print(result.graph_break_count)\n",
        "print(result.break_reasons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: torch.Size([10, 10])\n",
            "1\n",
            "[GraphCompileReason(reason=\"Failed to trace builtin operator\\n  Explanation: Dynamo does not know how to trace builtin operator `print` with argument types ['str'] (has_kwargs False)\\n  Hint: Avoid calling builtin `print` with argument types ['str']. Consider using an equivalent alternative function/method to `print`.\\n  Hint: If you are attempting to call a logging function (e.g. `print`), you can try adding it to `torch._dynamo.config.reorderable_logging_functions`.\\n  Hint: Please report an issue to PyTorch.\\n\\n  Developer debug context: builtin print [<class 'torch._dynamo.variables.constant.ConstantVariable'>] False\\n\\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0059.html\", user_stack=[<FrameSummary file /tmp/ipykernel_1912855/2490334730.py, line 5 in fn_with_print>], graph_break=True)]\n"
          ]
        }
      ],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "def fn_with_print(x):\n",
        "    x = x * 2\n",
        "    print(f\"Shape: {x.shape}\")\n",
        "    x = x + 1\n",
        "    return x\n",
        "\n",
        "explanation = torch._dynamo.explain(fn_with_print)\n",
        "x = torch.randn(10, 10, device=device)\n",
        "result = explanation(x)\n",
        "print(result.graph_break_count)\n",
        "print(result.break_reasons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0217 10:19:46.763000 1912855 torch/_dynamo/variables/tensor.py:1079] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
            "W0217 10:19:46.763000 1912855 torch/_dynamo/variables/tensor.py:1079] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
            "W0217 10:19:46.763000 1912855 torch/_dynamo/variables/tensor.py:1079] [0/0] or:\n",
            "W0217 10:19:46.763000 1912855 torch/_dynamo/variables/tensor.py:1079] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
            "W0217 10:19:46.763000 1912855 torch/_dynamo/variables/tensor.py:1079] [0/0] to include these operations in the captured graph.\n",
            "W0217 10:19:46.763000 1912855 torch/_dynamo/variables/tensor.py:1079] [0/0] \n",
            "W0217 10:19:46.763000 1912855 torch/_dynamo/variables/tensor.py:1079] [0/0] Graph break: from user code at:\n",
            "W0217 10:19:46.763000 1912855 torch/_dynamo/variables/tensor.py:1079] [0/0]   File \"/tmp/ipykernel_1912855/4059112263.py\", line 5, in fn_with_item\n",
            "W0217 10:19:46.763000 1912855 torch/_dynamo/variables/tensor.py:1079] [0/0]     val = x[0, 0].item()\n",
            "W0217 10:19:46.763000 1912855 torch/_dynamo/variables/tensor.py:1079] [0/0] \n",
            "W0217 10:19:46.763000 1912855 torch/_dynamo/variables/tensor.py:1079] [0/0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "[GraphCompileReason(reason='Unsupported Tensor.item() call with capture_scalar_outputs=False\\n  Explanation: Dynamo does not support tracing `Tensor.item()` with config.capture_scalar_outputs=False.\\n  Hint: Set `torch._dynamo.config.capture_scalar_outputs = True` or `export TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` to include these operations in the captured graph.\\n\\n  Developer debug context: call_method TensorVariable() item () {}\\n\\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0124.html', user_stack=[<FrameSummary file /tmp/ipykernel_1912855/4059112263.py, line 5 in fn_with_item>], graph_break=True)]\n"
          ]
        }
      ],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "def fn_with_item(x):\n",
        "    x = x * 2\n",
        "    val = x[0, 0].item()\n",
        "    if val > 0:\n",
        "        x = x + 1\n",
        "    return x\n",
        "\n",
        "explanation = torch._dynamo.explain(fn_with_item)\n",
        "x = torch.randn(10, 10, device=device)\n",
        "result = explanation(x)\n",
        "print(result.graph_break_count)\n",
        "print(result.break_reasons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "[GraphCompileReason(reason='Tensor.tolist() with non-integer tensor\\n  Explanation: Dynamo currently does not support tracing `tolist()` on non-integer tensors.\\n  Hint: Ensure the input tensor to `tolist()` is an integer type (e.g., int8, int16, int32, int64).\\n\\n  Developer debug context: call_method TensorVariable() to_list\\n\\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0109.html', user_stack=[<FrameSummary file /tmp/ipykernel_1912855/168308132.py, line 6 in fn_with_tolist>], graph_break=True)]\n"
          ]
        }
      ],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "def fn_with_tolist(x):\n",
        "    x = x * 2\n",
        "    shape_list = list(x.shape)\n",
        "    data_list = x[0].tolist()\n",
        "    x = x + 1\n",
        "    return x\n",
        "\n",
        "explanation = torch._dynamo.explain(fn_with_tolist)\n",
        "x = torch.randn(10, 10, device=device)\n",
        "result = explanation(x)\n",
        "print(result.graph_break_count)\n",
        "print(result.break_reasons)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extra. The 3 stages of torch.compile.\n",
        "\n",
        "1. **Graph Acquisition (TorchDynamo + AOTAutograd)** - TorchDynamo intercepts Python bytecode at runtime and extracts the computational operations into an FX graph. AOTAutograd then traces both forward and backward passes ahead-of-time, producing separate graphs for each.\n",
        "\n",
        "2. **Graph Lowering** - The high-level FX graph is lowered into a more primitive representation. Operations are decomposed into simpler ops, and the graph is normalized into a form suitable for optimization and code generation.\n",
        "\n",
        "3. **Graph Compilation (TorchInductor)** - The backend compiler takes the lowered graph and generates optimized kernels. TorchInductor produces Triton code for GPU, applying optimizations like fusion, memory planning, and efficient scheduling.\n",
        "\n",
        "Let's interactively explore what happens at each stage when `compiled_fn(x)` runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stage 1: Graph Acquisition (TorchDynamo)\n",
        "\n",
        "TorchDynamo captures the Python bytecode and builds an FX graph. We can inspect this using a custom backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "graph():\n",
            "    %l_x_ : torch.Tensor [num_users=1] = placeholder[target=L_x_]\n",
            "    %x : [num_users=1] = call_function[target=operator.mul](args = (%l_x_, 2), kwargs = {})\n",
            "    %x_1 : [num_users=1] = call_function[target=operator.add](args = (%x, 1), kwargs = {})\n",
            "    %x_2 : [num_users=1] = call_function[target=torch.relu](args = (%x_1,), kwargs = {})\n",
            "    %x_3 : [num_users=1] = call_function[target=operator.mul](args = (%x_2, 0.5), kwargs = {})\n",
            "    return (x_3,)\n"
          ]
        }
      ],
      "source": [
        "def inspect_graph(gm, example_inputs):\n",
        "    \"\"\"\n",
        "    Custom backend that prints the FX graph captured by Dynamo.\n",
        "    \n",
        "    Args:\n",
        "        gm: torch.fx.GraphModule containing the captured FX graph.\n",
        "            - gm.graph: the FX graph representation\n",
        "            - gm.forward: callable to execute the graph\n",
        "        example_inputs: List[Tensor] - the actual inputs that triggered compilation\n",
        "    \n",
        "    Returns:\n",
        "        A callable that executes the graph (here we just return eager execution)\n",
        "    \"\"\"\n",
        "    print(gm.graph)\n",
        "    return gm.forward\n",
        "\n",
        "# The `backend` parameter specifies which compiler will process the captured graph.\n",
        "# The default is `\"inductor\"` (TorchInductor), which generates optimized Triton/C++ kernels.\n",
        "torch._dynamo.reset()\n",
        "inspect_compiled = torch.compile(simple_fn, backend=inspect_graph)\n",
        "\n",
        "x = torch.randn(1000, 1000, device=device)\n",
        "_ = inspect_compiled(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**AOTAutograd: Forward and Backward Graphs**\n",
        "\n",
        "The graph above is just what Dynamo captured. AOTAutograd then takes this and generates separate forward and backward graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Forward Graph ===\n",
            "graph():\n",
            "    %primals_1 : [num_users=1] = placeholder[target=primals_1]\n",
            "    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%primals_1, 2), kwargs = {})\n",
            "    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul, 1), kwargs = {})\n",
            "    %relu : [num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%add,), kwargs = {})\n",
            "    %detach : [num_users=1] = call_function[target=torch.ops.aten.detach.default](args = (%relu,), kwargs = {})\n",
            "    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%relu, 0.5), kwargs = {})\n",
            "    return (mul_1, detach)\n",
            "\n",
            "=== Backward Graph ===\n",
            "graph():\n",
            "    %detach : [num_users=1] = placeholder[target=detach]\n",
            "    %tangents_1 : [num_users=1] = placeholder[target=tangents_1]\n",
            "    %mul_2 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%tangents_1, 0.5), kwargs = {})\n",
            "    %detach_1 : [num_users=1] = call_function[target=torch.ops.aten.detach.default](args = (%detach,), kwargs = {})\n",
            "    %threshold_backward : [num_users=1] = call_function[target=torch.ops.aten.threshold_backward.default](args = (%mul_2, %detach_1, 0), kwargs = {})\n",
            "    %mul_3 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%threshold_backward, 2), kwargs = {})\n",
            "    return (mul_3,)\n"
          ]
        }
      ],
      "source": [
        "from torch._functorch.aot_autograd import aot_function\n",
        "from functorch.compile import make_boxed_func\n",
        "\n",
        "def fw_compiler(gm, example_inputs):\n",
        "    print(\"=== Forward Graph ===\")\n",
        "    print(gm.graph)\n",
        "    print()\n",
        "    return make_boxed_func(gm.forward)\n",
        "\n",
        "def bw_compiler(gm, example_inputs):\n",
        "    print(\"=== Backward Graph ===\")\n",
        "    print(gm.graph)\n",
        "    return make_boxed_func(gm.forward)\n",
        "\n",
        "aot_fn = aot_function(simple_fn, fw_compiler=fw_compiler, bw_compiler=bw_compiler)\n",
        "\n",
        "x = torch.randn(1000, 1000, device=device, requires_grad=True)\n",
        "\n",
        "out = aot_fn(x)\n",
        "\n",
        "out.sum().backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stage 2: Graph Lowering (Decomposition)\n",
        "\n",
        "The FX graph is then lowered - high-level ops are decomposed into primitives. We can see this using `torch._decomp`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "graph():\n",
            "    %x_1 : [num_users=1] = placeholder[target=x_1]\n",
            "    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%x_1, 2), kwargs = {})\n",
            "    %add : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul, 1), kwargs = {})\n",
            "    %le : [num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%add, 0), kwargs = {})\n",
            "    %scalar_tensor : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (0,), kwargs = {dtype: torch.float32, layout: torch.strided, device: cuda:0})\n",
            "    %where : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%le, %scalar_tensor, %add), kwargs = {})\n",
            "    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%where, 0.5), kwargs = {})\n",
            "    return mul_1\n"
          ]
        }
      ],
      "source": [
        "from torch.fx.experimental.proxy_tensor import make_fx\n",
        "from torch._decomp import get_decompositions\n",
        "\n",
        "decompositions = get_decompositions([\n",
        "    torch.ops.aten.relu,\n",
        "])\n",
        "\n",
        "x = torch.randn(1000, 1000, device=device)\n",
        "lowered_graph = make_fx(simple_fn, decomposition_table=decompositions)(x)\n",
        "\n",
        "print(lowered_graph.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stage 3: Graph Compilation (TorchInductor)\n",
        "\n",
        "Finally, TorchInductor generates optimized Triton kernels. We can see the generated code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] Output code: \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # AOT ID: ['0_inference']\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import torch\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import math\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import random\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import os\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import tempfile\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from math import inf, nan\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from cmath import nanj\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.utils import maybe_profile\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch import device, empty_strided\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import triton\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import triton.language as tl\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] aten = torch.ops.aten\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] inductor_ops = torch.ops.inductor\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] _quantized = torch.ops._quantized\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] async_compile = AsyncCompile()\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # kernel path: /tmp/torchinductor_mabraham/mb/cmbenptznauc4m7cfeb2iuwxjyr3gybi3jnimhqiztiy6v57afy6.py\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # Topologically Sorted Source Nodes: [x, x_1, x_2, x_3], Original ATen: [aten.mul, aten.add, aten.relu]\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # Source node to ATen node mapping:\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   x => mul\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   x_1 => add\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   x_2 => relu\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   x_3 => mul_1\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] # Graph fragment:\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %arg0_1 : Tensor \"f32[1000, 1000][1000, 1]cuda:0\" = PlaceHolder[target=arg0_1]\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %mul : Tensor \"f32[1000, 1000][1000, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, 2), kwargs = {})\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %add : Tensor \"f32[1000, 1000][1000, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul, 1), kwargs = {})\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %relu : Tensor \"f32[1000, 1000][1000, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%add,), kwargs = {})\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   %mul_1 : Tensor \"f32[1000, 1000][1000, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%relu, 0.5), kwargs = {})\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] #   return %mul_1\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] triton_poi_fused_add_mul_relu_0 = async_compile.triton('triton_poi_fused_add_mul_relu_0', '''\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import triton\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] import triton.language as tl\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] @triton_heuristics.pointwise(\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     size_hints={'x': 1048576}, \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     filename=__file__,\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, max_threads_per_block=1024, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_relu_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 1, 'num_store': 1, 'num_reduction': 0, 'backend_hash': '91D1D1166449D9E2EB66EB9770C9E2C3B0FEB787341F27FEB4D3F0338DA50336', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 12000000}},\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     min_elem_per_thread=0\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] )\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] @triton.jit\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] def triton_poi_fused_add_mul_relu_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     xnumel = 1000000\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     xmask = xindex < xnumel\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     x0 = xindex\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), xmask)\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp1 = 2.0\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp2 = tmp0 * tmp1\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp3 = 1.0\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp4 = tmp2 + tmp3\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp5 = tl.full([1], 0, tl.int32)\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp6 = triton_helpers.maximum(tmp5, tmp4)\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp7 = 0.5\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tmp8 = tmp6 * tmp7\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp8, xmask)\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] ''', device_str='cuda')\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] async_compile.wait(globals())\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] del async_compile\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] class Runner:\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     def __init__(self, partitions):\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         self.partitions = partitions\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     def recursively_apply_fns(self, fns):\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         new_callables = []\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             new_callables.append(fn(c))\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         self.partitions = new_callables\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     def call(self, args):\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         arg0_1, = args\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         args.clear()\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         assert_size_stride(arg0_1, (1000, 1000), (1000, 1))\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             torch.cuda.set_device(0)\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             buf0 = empty_strided_cuda((1000, 1000), (1000, 1), torch.float32)\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             # Topologically Sorted Source Nodes: [x, x_1, x_2, x_3], Original ATen: [aten.mul, aten.add, aten.relu]\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             stream0 = get_raw_stream(0)\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             triton_poi_fused_add_mul_relu_0.run(arg0_1, buf0, 1000000, stream=stream0)\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]             del arg0_1\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]         return (buf0, )\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] runner = Runner(partitions=[])\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] call = runner.call\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     from torch._inductor.utils import print_performance\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     arg0_1 = rand_strided((1000, 1000), (1000, 1), device='cuda:0', dtype=torch.float32)\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     fn = lambda: call([arg0_1])\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] if __name__ == \"__main__\":\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\n",
            "V0216 10:53:11.711000 831728 torch/_inductor/codecache.py:1250] [0/0] [__output_code] \n",
            "V0216 10:53:11.713000 831728 torch/_inductor/codecache.py:1251] [0/0] [__output_code] Output code written to: /tmp/torchinductor_mabraham/mo/cmoanw7n3xnuq33pwdrixobi2t67elh4kgfhjvn7pvkzgmxnknqb.py\n"
          ]
        }
      ],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "torch._logging.set_logs(output_code=True)\n",
        "\n",
        "compiled_with_debug = torch.compile(simple_fn)\n",
        "\n",
        "x = torch.randn(1000, 1000, device=device)\n",
        "_ = compiled_with_debug(x)\n",
        "\n",
        "torch._logging.set_logs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For further reading:\n",
        "1. [Torch Compiler Troubleshooting](https://docs.pytorch.org/docs/stable/user_guide/torch_compiler/torch.compiler_troubleshooting.html)\n",
        "2. [The Missing Manual](https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0#heading=h.ivdr7fmrbeab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From Elementwise to Matmuls\n",
        "\n",
        "We've seen that elementwise operations like SiLU are **memory-bound** - each element requires memory access with minimal computation.\n",
        "\n",
        "But what about **matrix multiplication**? \n",
        "\n",
        "In the lecture, when calculating memory time for `matmul(A, B)` with `(N, N)` matrices in bf16, we used:\n",
        "\n",
        "$$T_{memory} = \\frac{N \\times N \\times 3 \\times 2}{\\text{bandwidth}}$$\n",
        "\n",
        "Where:\n",
        "- $N \\times N$ - number of elements\n",
        "- $3$ - two reads (A, B) and one write (C)  \n",
        "- $2$ - bytes per bf16 element\n",
        "\n",
        "**But wait...**\n",
        "\n",
        "To compute a single output element $C[i,j] = \\sum_k A[i,k] \\cdot B[k,j]$, we need to read an entire row of A and entire column of B - that's $2N$ memory reads per output element. For all $N^2$ output elements, shouldn't memory be $2N^3$ reads, not just $3N^2$?\n",
        "\n",
        "**Why do we only count each matrix element once?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. GPU Memory Hierarchy\n",
        "\n",
        "![Memory Hierarchy](images/mem_hierarchy.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cache Behavior in Matmul\n",
        "\n",
        "Let's check cache hit rates for matrix multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Profile with Nsight Compute:\n",
        "!ncu --metrics lts__t_sector_hit_rate.pct python profile_matmul.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Nsight Compute Results:**\n",
        "<pre style=\"background-color: #f5f5f5; color: #333; padding: 12px; border-radius: 6px; font-family: monospace;\">\n",
        "<span style=\"color: #569cd6;\">sm80_xmma_gemm_f32f32_f32f32_f32_nn_n...</span> (16, 32, 1)x(256, 1, 1)\n",
        "Device 0, CC 9.0, Invocations 10\n",
        "\n",
        "Metric                       Min       Max       Avg\n",
        "âââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
        "lts__t_sector_hit_rate.pct   89.36%    90.40%    <b>89.80%</b>\n",
        "</pre>\n",
        "\n",
        "Why ~90% for matmul? Each element of A and B is loaded once from HBM, then reused ~N times from L2/L1 cache for computing multiple output elements. The tiled algorithm ensures high data reuse within a single kernel.\n",
        "\n",
        "This is why we count memory as $3N^2$ (like we load it once)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO example with cache polluting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Efficient Cross Entropy\n",
        "\n",
        "### The Cross Entropy Memory Problem\n",
        "\n",
        "**Notation:**\n",
        "- N = batch_size Ã sequence_length (number of tokens)\n",
        "- V = vocabulary size\n",
        "\n",
        "**Standard Cross Entropy Memory:**\n",
        "\n",
        "Forward pass:\n",
        "- `logits` (N, V) - input from final linear layer\n",
        "- `log_softmax(logits)` (N, V) - saved for backward\n",
        "\n",
        "Backward pass:\n",
        "- `softmax = exp(log_softmax)` (N, V) - needed to compute gradient\n",
        "- `grad_logits` (N, V) - output gradient: `softmax - one_hot(target)`\n",
        "\n",
        "**Peak memory:** 4x (N, V) tensors during backward!\n",
        "\n",
        "Example: bf16 with shape `(1, 32768, 128000)` â **8 GB per tensor**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Liger Kernel Cross Entropy\n",
        "\n",
        "Code: [Liger Kernel Cross Entropy](https://github.com/linkedin/Liger-Kernel/blob/main/src/liger_kernel/ops/cross_entropy.py).\n",
        "\n",
        "**Key Optimizations:**\n",
        "\n",
        "**1. In-place Gradient Storage**\n",
        "\n",
        "Instead of allocating a separate gradient tensor, store the gradient directly in the input logits:\n",
        "\n",
        "$$\\nabla_x L = \\text{softmax}(x) - \\text{one\\_hot}(\\text{target})$$\n",
        "\n",
        "```python\n",
        "# Forward pass computes both: loss and gradient\n",
        "# Gradient overwrites the logits in-place\n",
        "logits[i] = softmax(logits[i]) - (1 if i == target else 0)\n",
        "```\n",
        "\n",
        "**2. Online Softmax**\n",
        "\n",
        "Compute softmax statistics in a streaming fashion without materializing the full probability vector:\n",
        "\n",
        "```python\n",
        "m, d = -inf, 0.0  # running max and denominator\n",
        "for chunk in blocks(logits):\n",
        "    m_new = max(m, chunk.max())\n",
        "    d = d * exp(m - m_new) + sum(exp(chunk - m_new))\n",
        "    m = m_new\n",
        "\n",
        "lse = m + log(d)  # log-sum-exp\n",
        "loss = lse - logits[target]  # = -log(softmax[target])\n",
        "```\n",
        "\n",
        "Result: Peak memory reduced from 4x to 1x (N, V).\n",
        "\n",
        "Let's take a look at snapshots now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 week06/cross_entropy/profile_vanilla_ce.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Vanilla Snapshot](images/vanilla_ce.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 week06/cross_entropy/compiled_ce_snapshot.py "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Compiled Snapshot](images/compiled_ce.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 week06/cross_entropy/profile_liger_ce.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Vanilla Snapshot](images/liger_ce.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fused Linear Cross Entropy (FLCE)\n",
        "\n",
        "Liger CE reduces peak memory from 4x to 1x, but still materializes the full logits tensor. FLCE avoids this by fusing the linear projection with cross-entropy computation.\n",
        "\n",
        "Code: [Liger Fused Linear Cross Entropy](https://github.com/linkedin/Liger-Kernel/blob/main/src/liger_kernel/ops/fused_linear_cross_entropy.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Liger Fused Linear Cross Entropy](images/liger_fused.avif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For further optimization of cross-entropy with large vocabularies, see Apple's Cut Cross-Entropy: [Cut Your Losses\n",
        "in Large-Vocabulary Language Models](https://arxiv.org/pdf/2411.09009v1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "week06",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
