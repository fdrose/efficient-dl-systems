{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 6: kernel fusion, torch.compile, GPU memory and Liger kernels\n",
        "\n",
        "### Seminar outline\n",
        "1. Kernel Fusion\n",
        "    - Why fusing operations matters\n",
        "    - SwiGLU fusion\n",
        "2. Internals of torch.compile\n",
        "    - Basic torch.compile example\n",
        "    - Understanding What Dynamo Captures\n",
        "    - Graph Breaks\n",
        "    - Loops\n",
        "    - Extra. The 3 stages of torch.compile.\n",
        "        - TorchDynamo: bytecode capture and FX graphs\n",
        "        - AOTAutograd: forward and backward tracing\n",
        "        - TorchInductor: Triton/C++ kernel generation\n",
        "3. GPU Memory Hierarchy\n",
        "4. Efficient Cross Entropy\n",
        "    - Liger Kernel Cross Entropy\n",
        "    - Fused Linear Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Kernel Fusion\n",
        "\n",
        "### Why Kernel Fusion Matters\n",
        "\n",
        "**Problem:**\n",
        "- GPU compute is much faster than memory\n",
        "- For H100: 800 TFLOPS (25 TFLOPS for CUDA Cores) compute vs 2.4 TB/s memory bandwidth\n",
        "- For elementwise ops, we're almost always memory-bound\n",
        "\n",
        "**Why Fusion Helps:**\n",
        "- Reduces memory traffic:\n",
        "    - Unfused ops read/write HBM for each operation\n",
        "    - Fused ops keep intermediates in registers\n",
        "- Reduces kernel launch overhead:\n",
        "    - Each CUDA kernel launch has ~5-10μs overhead\n",
        "    - Fusing N ops into 1 kernel eliminates N-1 launches\n",
        "\n",
        "<p float=\"left\">\n",
        "<img src=\"./images/fused_kernels1.png\" width=\"400\"/>\n",
        "<img src=\"./images/fused_kernels2.png\" width=\"400\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SwiGLU fusion\n",
        "\n",
        "SwiGLU is used in modern LLMs:\n",
        "\n",
        "$$\\text{SwiGLU}(x, y) = x \\odot \\text{SiLU}(y), \\quad \\text{SiLU}(x) = x \\cdot \\sigma(x)$$\n",
        "\n",
        "The elementwise multiplication and SiLU are perfect fusion candidates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "from liger_kernel.ops.utils import calculate_settings\n",
        "\n",
        "\n",
        "def swiglu_unfused(gate, up):\n",
        "    \"\"\"Each operation launches a separate CUDA kernel\"\"\"\n",
        "    return F.silu(gate) * up\n",
        "\n",
        "\n",
        "@torch.compile\n",
        "def swiglu_compiled(gate, up):\n",
        "    \"\"\"torch.compile fuses silu + mul into a single kernel\"\"\"\n",
        "    return F.silu(gate) * up\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _silu(x):\n",
        "    return x * tl.sigmoid(x)\n",
        "\n",
        "@triton.jit\n",
        "def _swiglu_forward_kernel(a_ptr, b_ptr, c_ptr, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n",
        "    program_id = tl.program_id(0).to(tl.int64)\n",
        "\n",
        "    # locate start index\n",
        "    a_ptr += program_id * stride\n",
        "    b_ptr += program_id * stride\n",
        "    c_ptr += program_id * stride\n",
        "\n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
        "    mask = col_offsets < n_cols\n",
        "\n",
        "    # sigmoid requires type float32\n",
        "    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)\n",
        "    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n",
        "    c_row = _silu(a_row) * b_row\n",
        "    tl.store(c_ptr + col_offsets, c_row, mask=mask)\n",
        "\n",
        "\n",
        "def swiglu_forward(a, b):\n",
        "    ori_shape = a.shape\n",
        "\n",
        "    n_cols = ori_shape[-1]\n",
        "    a = a.view(-1, n_cols)\n",
        "    b = b.view(-1, n_cols)\n",
        "    c = torch.empty_like(a)\n",
        "    n_rows = a.shape[0]\n",
        "\n",
        "    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n",
        "\n",
        "    _swiglu_forward_kernel[(n_rows,)](\n",
        "        a,\n",
        "        b,\n",
        "        c,\n",
        "        c.stride(-2),\n",
        "        n_cols=n_cols,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "        num_warps=num_warps,\n",
        "    )\n",
        "    return a, b, c.view(*ori_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def benchmark(fn, gate, up, warmup=10, iters=1000):\n",
        "    for _ in range(warmup):\n",
        "        _ = fn(gate, up)\n",
        "\n",
        "    start_event = torch.cuda.Event(enable_timing=True)\n",
        "    end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start_event.record()\n",
        "\n",
        "    for _ in range(iters):\n",
        "        _ = fn(gate, up)\n",
        "\n",
        "    end_event.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    return start_event.elapsed_time(end_event) / iters\n",
        "\n",
        "\n",
        "gate = torch.randn(4096, 4096, device=device)\n",
        "up = torch.randn(4096, 4096, device=device)\n",
        "\n",
        "for _ in range(10):\n",
        "    _ = swiglu_unfused(gate, up)\n",
        "    _ = swiglu_compiled(gate, up)\n",
        "    _ = swiglu_forward(gate, up)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "print(f\"Unfused:       {benchmark(swiglu_unfused, gate, up):.3f} ms\")\n",
        "print(f\"torch.compile: {benchmark(swiglu_compiled, gate, up):.3f} ms\")\n",
        "print(f\"Triton:        {benchmark(swiglu_forward, gate, up):.3f} ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**OpenAI gpt-oss SwiGLU variant:**\n",
        "\n",
        "$$\\text{SwiGLU}_{\\text{gpt-oss}}(x, y) = x \\cdot \\sigma(\\alpha x) \\cdot (y + 1)$$\n",
        "\n",
        "Using the identity $\\sigma(z) = \\frac{1}{2}(1 + \\tanh(\\frac{z}{2}))$, this becomes:\n",
        "\n",
        "$$\\frac{x}{2} \\cdot \\left(1 + \\tanh\\left(\\frac{\\alpha x}{2}\\right)\\right) \\cdot (y + 1)$$\n",
        "\n",
        "**Optimized form (5 ops: FMUL, FMUL, TANH, FFMA, FFMA):**\n",
        "\n",
        "$$\\text{Let } h = \\frac{x}{2}, \\quad s = h \\cdot \\tanh(\\alpha h) + h$$\n",
        "$$\\text{SwiGLU}_{\\text{gpt-oss}}(x, y) = s \\cdot y + s$$\n",
        "\n",
        "```\n",
        "h = x * 0.5           // FMUL #1\n",
        "t = alpha * h         // FMUL #2  \n",
        "t = tanh(t)           // TANH\n",
        "s = h * t + h         // FFMA #1  (fused multiply-add)\n",
        "out = s * y + s       // FFMA #2  (fused multiply-add)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ALPHA = 1.0\n",
        "\n",
        "def swiglu_gptoss_unfused(x, y):\n",
        "    \"\"\"GPT-OSS style: x * sigmoid(a*x) * (y + 1) - unfused\"\"\"\n",
        "    return x * torch.sigmoid(ALPHA * x) * (y + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch._dynamo.reset()\n",
        "torch._logging.set_logs(output_code=True)\n",
        "\n",
        "@torch.compile\n",
        "def swiglu_gptoss_inspect(x, y):\n",
        "    return x * torch.sigmoid(ALPHA * x) * (y + 1)\n",
        "\n",
        "x = torch.randn(4096, 4096, device=device)\n",
        "y = torch.randn(4096, 4096, device=device)\n",
        "_ = swiglu_gptoss_inspect(x, y)\n",
        "\n",
        "torch._logging.set_logs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch._dynamo.reset()\n",
        "torch._logging.set_logs(output_code=True)\n",
        "\n",
        "@torch.compile\n",
        "def swiglu_gptoss_tanh_compiled(x, y):\n",
        "    x_half = 0.5 * x\n",
        "    silu_x = x_half * torch.tanh(ALPHA * x_half) + x_half\n",
        "    return silu_x * y + silu_x\n",
        "\n",
        "x = torch.randn(4096, 4096, device=device)\n",
        "y = torch.randn(4096, 4096, device=device)\n",
        "_ = swiglu_gptoss_tanh_compiled(x, y)\n",
        "\n",
        "torch._logging.set_logs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn(4096, 4096, device=device)\n",
        "y = torch.randn(4096, 4096, device=device)\n",
        "\n",
        "print(f\"Unfused (eager):     {benchmark(swiglu_gptoss_unfused, x, y):.3f} ms\")\n",
        "print(f\"Compiled (sigmoid):  {benchmark(swiglu_gptoss_inspect, x, y):.3f} ms\")\n",
        "print(f\"Compiled (tanh):     {benchmark(swiglu_gptoss_tanh_compiled, x, y):.3f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Memory Traffic Analysis\n",
        "\n",
        "For inputs `gate` and `up` of shape `(batch, seq, hidden_dim)`:\n",
        "\n",
        "**Unfused SwiGLU:**\n",
        "\n",
        "| Step | Operation | Memory Access |\n",
        "|------|-----------|---------------|\n",
        "| 1 | sigmoid(gate) | read, write |\n",
        "| 2 | gate * sigmoid | 2x read, write |\n",
        "| 3 | silu * up | 2x read, write |\n",
        "| **Total** | **3 kernels** | **8 * (batch, seq, hidden_dim) reads/writes** |\n",
        "\n",
        "**Fused SwiGLU:**\n",
        "\n",
        "| Step | Operation | Memory Access |\n",
        "|------|-----------|---------------|\n",
        "| 1 | silu(gate) * up | 2x read, write |\n",
        "| **Total** | **1 kernel** | **3 * (batch, seq, hidden_dim) reads/writes** |\n",
        "\n",
        "All operations (sigmoid, multiply, multiply) happen in registers - no intermediate writes to HBM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H100 SXM specs\n",
        "MEMORY_BANDWIDTH_TB_S = 2.4\n",
        "CUDA_CORE_TFLOPS = 25\n",
        "\n",
        "def calculate_theoretical_time(\n",
        "    shape, \n",
        "    n_reads, \n",
        "    n_writes, \n",
        "    flops_per_element,\n",
        "    dtype_bytes=4\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate theoretical execution time based on memory and compute.\n",
        "    \n",
        "    Args:\n",
        "        shape: tensor shape\n",
        "        n_reads: number of tensor reads\n",
        "        n_writes: number of tensor writes  \n",
        "        flops_per_element: FLOPs per element (e.g., mul=1, add=1, sigmoid≈10)\n",
        "        dtype_bytes: bytes per element (FP32=4, bf16=2)\n",
        "    \n",
        "    Returns:\n",
        "        memory_time_ms, compute_time_ms, is_memory_bound\n",
        "    \"\"\"\n",
        "    n_elements = 1\n",
        "    for dim in shape:\n",
        "        n_elements *= dim\n",
        "\n",
        "    total_bytes = (n_reads + n_writes) * n_elements * dtype_bytes\n",
        "    total_gb = total_bytes / 1e9\n",
        "    memory_time_ms = total_gb / (MEMORY_BANDWIDTH_TB_S * 1000) * 1000  # TB/s → GB/ms\n",
        "\n",
        "    total_flops = n_elements * flops_per_element\n",
        "    total_tflops = total_flops / 1e12\n",
        "    compute_time_ms = total_tflops / CUDA_CORE_TFLOPS * 1000\n",
        "\n",
        "    is_memory_bound = memory_time_ms > compute_time_ms\n",
        "    return memory_time_ms, compute_time_ms, is_memory_bound\n",
        "\n",
        "\n",
        "shape = (4096, 4096)\n",
        "\n",
        "# Unfused: 3 separate kernels\n",
        "# Kernel 1: sigmoid(gate) - read 1, write 1, ~10 FLOPs (exp, div, etc.)\n",
        "# Kernel 2: gate * sigmoid - read 2, write 1, 1 FLOP\n",
        "# Kernel 3: silu * up - read 2, write 1, 1 FLOP\n",
        "unfused_reads = 1 + 2 + 2  # = 5\n",
        "unfused_writes = 1 + 1 + 1  # = 3\n",
        "unfused_flops = 10 + 1 + 1  # = 12\n",
        "\n",
        "mem_t, comp_t, is_mem = calculate_theoretical_time(shape, unfused_reads, unfused_writes, unfused_flops)\n",
        "print(f\"\\n**Unfused:**\")\n",
        "print(f\"  Memory: {unfused_reads} reads + {unfused_writes} writes = {unfused_reads + unfused_writes}x tensor\")\n",
        "print(f\"  Memory time:  {mem_t:.4f} ms\")\n",
        "print(f\"  Compute time: {comp_t:.4f} ms\")\n",
        "print(f\"  Bound by: {'MEMORY' if is_mem else 'COMPUTE'} ({mem_t/comp_t:.1f}x ratio)\")\n",
        "\n",
        "fused_reads = 2\n",
        "fused_writes = 1\n",
        "fused_flops = 12\n",
        "\n",
        "mem_t, comp_t, is_mem = calculate_theoretical_time(shape, fused_reads, fused_writes, fused_flops)\n",
        "print(f\"\\n**Fused:**\")\n",
        "print(f\"  Memory: {fused_reads} reads + {fused_writes} writes = {fused_reads + fused_writes}x tensor\")\n",
        "print(f\"  Memory time:  {mem_t:.4f} ms\")\n",
        "print(f\"  Compute time: {comp_t:.4f} ms\")\n",
        "print(f\"  Bound by: {'MEMORY' if is_mem else 'COMPUTE'} ({mem_t/comp_t:.1f}x ratio)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Internals of torch.compile\n",
        "\n",
        "The [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html) allows you to compile your existing PyTorch code into optimized kernels, automatically fusing operations like we saw in the previous section. It often achieves significant speedups with just a single line change: wrapping your model or function with `torch.compile()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic torch.compile example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_fn(x):\n",
        "    x = x * 2\n",
        "    x = x + 1\n",
        "    x = torch.relu(x)\n",
        "    x = x * 0.5\n",
        "    return x\n",
        "\n",
        "compiled_fn = torch.compile(simple_fn)\n",
        "\n",
        "x = torch.randn(1000, 1000, device=device)\n",
        "\n",
        "out1 = simple_fn(x)\n",
        "out2 = compiled_fn(x)  # First call triggers compilation\n",
        "\n",
        "print(f\"Results match: {torch.allclose(out1, out2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding What Dynamo Captures\n",
        "\n",
        "**TorchDynamo** is the first stage of `torch.compile`. It intercepts Python bytecode at runtime and extracts PyTorch operations into an FX graph - a simple intermediate representation that captures the sequence of tensor operations without Python overhead. For details, see the [Dynamo Deep-Dive](https://docs.pytorch.org/docs/stable/user_guide/torch_compiler/torch.compiler_dynamo_deepdive.html) documentation.\n",
        "\n",
        "We can use `torch._dynamo.explain()` to see what Dynamo captures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "explanation = torch._dynamo.explain(simple_fn)\n",
        "x = torch.randn(1000, 1000, device=device)\n",
        "\n",
        "result = explanation(x)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Graph Count: 1** - The entire function was captured in a single graph (good!)\n",
        "- **Graph Break Count: 0** - No graph breaks occurred (good!)\n",
        "- **Op Count: 4** - Four operations captured: `mul`, `add`, `relu`, `mul`\n",
        "- **Guards** - Conditions that must remain true for the cached compilation to be reused (e.g., tensor shapes, dtypes, device). If guards fail, Dynamo recompiles.\n",
        "\n",
        "| Guard | What it checks |\n",
        "|-------|----------------|\n",
        "| `SHAPE_ENV` | Symbolic shape constraints are satisfied |\n",
        "| `DETERMINISTIC_ALGORITHMS` | `torch.use_deterministic_algorithms()` unchanged |\n",
        "| `GRAD_MODE` | `torch.is_grad_enabled()` unchanged |\n",
        "| `DEFAULT_DEVICE` | Default device hasn't changed |\n",
        "| `GLOBAL_STATE` | Global PyTorch state unchanged |\n",
        "| `TORCH_FUNCTION_STATE` | `__torch_function__` dispatch unchanged |\n",
        "| `TENSOR_MATCH` | Input tensor properties match (shape, dtype, device, strides) |\n",
        "| `MODULE_MATCH` | The `torch` module is the same object |\n",
        "| `BUILTIN_MATCH` | `torch.relu` is the same builtin function |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graph Breaks\n",
        "\n",
        "**Graph breaks** occur when Dynamo encounters code it can't capture into the graph. Common causes:\n",
        "- **Data-dependent control flow** - value depends on tensor data\n",
        "- **Unsupported operations** - certain Python built-ins or dynamic features\n",
        "- **Non-compilable function calls** - functions Dynamo can't trace into\n",
        "\n",
        "When a graph break happens, Dynamo splits execution: compiled code runs up to the break, then Python takes over, then compilation may resume after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "def fn_with_break(x):\n",
        "    x = x * 2\n",
        "    if x.sum() > 0:\n",
        "        x = x + 1\n",
        "    else:\n",
        "        x = x - 1\n",
        "    return x\n",
        "\n",
        "explanation = torch._dynamo.explain(fn_with_break)\n",
        "x = torch.randn(10, 10, device=device)\n",
        "result = explanation(x)\n",
        "print(result.graph_break_count)\n",
        "print(result.break_reasons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "def fn_without_break(x):\n",
        "    x = x * 2\n",
        "    if x.shape[0] > 5:\n",
        "        x = x + 1\n",
        "    else:\n",
        "        x = x - 1\n",
        "    return x\n",
        "\n",
        "explanation = torch._dynamo.explain(fn_without_break)\n",
        "x = torch.randn(10, 10, device=device)\n",
        "result = explanation(x)\n",
        "print(result.graph_break_count)\n",
        "print(result.break_reasons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "def fn_with_print(x):\n",
        "    x = x * 2\n",
        "    print(f\"Shape: {x.shape}\")\n",
        "    x = x + 1\n",
        "    return x\n",
        "\n",
        "explanation = torch._dynamo.explain(fn_with_print)\n",
        "x = torch.randn(10, 10, device=device)\n",
        "result = explanation(x)\n",
        "print(result.graph_break_count)\n",
        "print(result.break_reasons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "def fn_with_item(x):\n",
        "    x = x * 2\n",
        "    val = x[0, 0].item()\n",
        "    if val > 0:\n",
        "        x = x + 1\n",
        "    return x\n",
        "\n",
        "explanation = torch._dynamo.explain(fn_with_item)\n",
        "x = torch.randn(10, 10, device=device)\n",
        "result = explanation(x)\n",
        "print(result.graph_break_count)\n",
        "print(result.break_reasons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "def fn_with_tolist(x):\n",
        "    x = x * 2\n",
        "    shape_list = list(x.shape)\n",
        "    data_list = x[0].tolist()\n",
        "    x = x + 1\n",
        "    return x\n",
        "\n",
        "explanation = torch._dynamo.explain(fn_with_tolist)\n",
        "x = torch.randn(10, 10, device=device)\n",
        "result = explanation(x)\n",
        "print(result.graph_break_count)\n",
        "print(result.break_reasons)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loop_update(tensors, value):\n",
        "    for t in tensors:\n",
        "        t.mul_(0.9).add_(value)\n",
        "\n",
        "def foreach_update(tensors, value):\n",
        "    torch._foreach_mul_(tensors, 0.9)\n",
        "    torch._foreach_add_(tensors, value)\n",
        "\n",
        "loop_compiled = torch.compile(loop_update)\n",
        "foreach_compiled = torch.compile(foreach_update)\n",
        "\n",
        "tensors_loop = [torch.randn(1000, 1000, device=\"cuda\") for _ in range(10)]\n",
        "tensors_foreach = [t.clone() for t in tensors_loop]\n",
        "\n",
        "torch._dynamo.reset()\n",
        "explanation = torch._dynamo.explain(loop_compiled)(tensors_loop, 0.1)\n",
        "print(f\"Loop version - Graph breaks: {explanation.graph_break_count}\")\n",
        "\n",
        "torch._dynamo.reset()\n",
        "explanation = torch._dynamo.explain(foreach_compiled)(tensors_foreach, 0.1)\n",
        "print(f\"Vectorized version - Graph breaks: {explanation.graph_break_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch._dynamo.reset()\n",
        "torch._logging.set_logs(output_code=True)\n",
        "\n",
        "def loop_update(tensors, value):\n",
        "    for t in tensors:\n",
        "        t.mul_(0.9).add_(value)\n",
        "\n",
        "def foreach_update(tensors, value):\n",
        "    torch._foreach_mul_(tensors, 0.9)\n",
        "    torch._foreach_add_(tensors, value)\n",
        "\n",
        "loop_compiled = torch.compile(loop_update)\n",
        "foreach_compiled = torch.compile(foreach_update)\n",
        "\n",
        "tensors_a = [torch.randn(1000, 1000, device=\"cuda\") for _ in range(5)]\n",
        "tensors_b = [t.clone() for t in tensors_a]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loop_compiled(tensors_a, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch._dynamo.reset()\n",
        "foreach_compiled(tensors_b, 0.1)\n",
        "\n",
        "torch._logging.set_logs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Python loop:      {benchmark(loop_update, tensors_loop, 0.1):.3f} ms\")\n",
        "print(f\"torch._foreach_:  {benchmark(foreach_update, tensors_foreach, 0.1):.3f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extra. The 3 stages of torch.compile.\n",
        "\n",
        "1. **Graph Acquisition (TorchDynamo + AOTAutograd)** - TorchDynamo intercepts Python bytecode at runtime and extracts the computational operations into an FX graph. AOTAutograd then traces both forward and backward passes ahead-of-time, producing separate graphs for each.\n",
        "\n",
        "2. **Graph Lowering** - The high-level FX graph is lowered into a more primitive representation. Operations are decomposed into simpler ops, and the graph is normalized into a form suitable for optimization and code generation.\n",
        "\n",
        "3. **Graph Compilation (TorchInductor)** - The backend compiler takes the lowered graph and generates optimized kernels. TorchInductor produces Triton code for GPU, applying optimizations like fusion, memory planning, and efficient scheduling.\n",
        "\n",
        "Let's interactively explore what happens at each stage when `compiled_fn(x)` runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stage 1: Graph Acquisition (TorchDynamo)\n",
        "\n",
        "TorchDynamo captures the Python bytecode and builds an FX graph. We can inspect this using a custom backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inspect_graph(gm, example_inputs):\n",
        "    \"\"\"\n",
        "    Custom backend that prints the FX graph captured by Dynamo.\n",
        "    \n",
        "    Args:\n",
        "        gm: torch.fx.GraphModule containing the captured FX graph.\n",
        "            - gm.graph: the FX graph representation\n",
        "            - gm.forward: callable to execute the graph\n",
        "        example_inputs: List[Tensor] - the actual inputs that triggered compilation\n",
        "    \n",
        "    Returns:\n",
        "        A callable that executes the graph (here we just return eager execution)\n",
        "    \"\"\"\n",
        "    print(gm.graph)\n",
        "    return gm.forward\n",
        "\n",
        "# The `backend` parameter specifies which compiler will process the captured graph.\n",
        "# The default is `\"inductor\"` (TorchInductor), which generates optimized Triton/C++ kernels.\n",
        "torch._dynamo.reset()\n",
        "inspect_compiled = torch.compile(simple_fn, backend=inspect_graph)\n",
        "\n",
        "x = torch.randn(1000, 1000, device=device)\n",
        "_ = inspect_compiled(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**AOTAutograd: Forward and Backward Graphs**\n",
        "\n",
        "The graph above is just what Dynamo captured. AOTAutograd then takes this and generates separate forward and backward graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch._functorch.aot_autograd import aot_function\n",
        "from functorch.compile import make_boxed_func\n",
        "\n",
        "def fw_compiler(gm, example_inputs):\n",
        "    print(\"=== Forward Graph ===\")\n",
        "    print(gm.graph)\n",
        "    print()\n",
        "    return make_boxed_func(gm.forward)\n",
        "\n",
        "def bw_compiler(gm, example_inputs):\n",
        "    print(\"=== Backward Graph ===\")\n",
        "    print(gm.graph)\n",
        "    return make_boxed_func(gm.forward)\n",
        "\n",
        "aot_fn = aot_function(simple_fn, fw_compiler=fw_compiler, bw_compiler=bw_compiler)\n",
        "\n",
        "x = torch.randn(1000, 1000, device=device, requires_grad=True)\n",
        "\n",
        "out = aot_fn(x)\n",
        "\n",
        "out.sum().backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stage 2: Graph Lowering (Decomposition)\n",
        "\n",
        "The FX graph is then lowered - high-level ops are decomposed into primitives. We can see this using `torch._decomp`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.fx.experimental.proxy_tensor import make_fx\n",
        "from torch._decomp import get_decompositions\n",
        "\n",
        "decompositions = get_decompositions([\n",
        "    torch.ops.aten.relu,\n",
        "])\n",
        "\n",
        "x = torch.randn(1000, 1000, device=device)\n",
        "lowered_graph = make_fx(simple_fn, decomposition_table=decompositions)(x)\n",
        "\n",
        "print(lowered_graph.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stage 3: Graph Compilation (TorchInductor)\n",
        "\n",
        "Finally, TorchInductor generates optimized Triton kernels. We can see the generated code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch._dynamo.reset()\n",
        "\n",
        "torch._logging.set_logs(output_code=True)\n",
        "\n",
        "compiled_with_debug = torch.compile(simple_fn)\n",
        "\n",
        "x = torch.randn(1000, 1000, device=device)\n",
        "_ = compiled_with_debug(x)\n",
        "\n",
        "torch._logging.set_logs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For further reading:\n",
        "1. [Torch Compiler Troubleshooting](https://docs.pytorch.org/docs/stable/user_guide/torch_compiler/torch.compiler_troubleshooting.html)\n",
        "2. [The Missing Manual](https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0#heading=h.ivdr7fmrbeab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From Elementwise to Matmuls\n",
        "\n",
        "We've seen that elementwise operations like SiLU are memory-bound - each element requires memory access with minimal computation.\n",
        "\n",
        "But what about matrix multiplication? \n",
        "\n",
        "In the lecture, when calculating memory time for `matmul(A, B)` with `(N, N)` matrices in bf16, we used:\n",
        "\n",
        "$$T_{memory} = \\frac{N \\times N \\times 3 \\times 2}{\\text{bandwidth}}$$\n",
        "\n",
        "Where:\n",
        "- $N \\times N$ - number of elements\n",
        "- $3$ - two reads (A, B) and one write (C)  \n",
        "- $2$ - bytes per bf16 element\n",
        "\n",
        "**But wait...**\n",
        "\n",
        "To compute a single output element $C[i,j] = \\sum_k A[i,k] \\cdot B[k,j]$, we need to read an entire row of A and entire column of B - that's $2N$ memory reads per output element. For all $N^2$ output elements, shouldn't memory be $2N^3$ reads, not just $3N^2$?\n",
        "\n",
        "**Why do we only count each matrix element once?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. GPU Memory Hierarchy\n",
        "\n",
        "Modern GPUs have a memory hierarchy, from fastest/smallest to slowest/largest:\n",
        "\n",
        "- **HBM (VRAM)** - The main GPU memory you see in specs.\n",
        "- **L2 Cache** - Shared across all SMs. Automatically caches HBM accesses (like CPU caches).\n",
        "- **L1 Cache / Shared Memory** - Per-SM fast memory. On modern GPUs, L1 and shared memory are unified.\n",
        "- **Registers** - Fastest storage, private to each thread. When we \"keep intermediates in registers\" for kernel fusion, this is what we mean.\n",
        "\n",
        "GPU caches work almost like CPU caches - when you access memory, a whole **cache line** (128 bytes) is fetched:\n",
        "- If data is in L1 - return immediately\n",
        "- If L1 miss, check L2 - load to L1 and return\n",
        "- If L2 miss - fetch from HBM into L2, then L1\n",
        "\n",
        "![Memory Hierarchy](images/mem_hierarchy.png)\n",
        "\n",
        "For further reading: [Inside NVIDIA GPUs: Anatomy of high performance matmul kernels](https://www.aleksagordic.com/blog/matmul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cache Behavior in Matmul\n",
        "\n",
        "Let's check cache hit rates for matrix multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Profile with Nsight Compute\n",
        "!ncu --metrics lts__t_sector_hit_rate.pct,dram__bytes_read.sum,dram__bytes_write.sum,lts__t_sectors.sum python profile_matmul.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Nsight Compute Results:**\n",
        "<pre style=\"background-color: #f5f5f5; color: #333; padding: 12px; border-radius: 6px; font-family: monospace;\">\n",
        "<span style=\"color: #569cd6;\">sm80_xmma_gemm_f32f32_f32f32_f32_nn_n...</span> (16, 32, 1)x(256, 1, 1)\n",
        "Device 0, CC 9.0\n",
        "\n",
        "Metric                       Metric Unit    Metric Value\n",
        "───────────────────────────────────────────────────────────\n",
        "dram__bytes_read.sum              Mbyte        <b>341.94</b>\n",
        "dram__bytes_write.sum             Mbyte         <b>53.44</b>\n",
        "lts__t_sector_hit_rate.pct            %        <b>89.53%</b>\n",
        "lts__t_sectors.sum               sector    <b>107,804,539</b>\n",
        "</pre>\n",
        "\n",
        "**Analysis:**\n",
        "- **DRAM reads**: 342 MB (inputs A, B with some cache misses, with 100% hitrate would be 128 MB)\n",
        "- **DRAM writes**: 53 MB (output C, ~64 MB with write coalescing)\n",
        "- **Total L2 traffic**: 107M sectors x 32 bytes = 3.45 GB\n",
        "- **Hit rate**: ~90% - only 10% of L2 accesses go to HBM\n",
        "\n",
        "Why ~90% of hitrate for matmul? Each element of A and B is loaded once from HBM, then reused ~N times from L2/L1 cache for computing multiple output elements. The tiled algorithm ensures high data reuse within a single kernel.\n",
        "\n",
        "This is why we count memory as $3N^2$ (like we load it once, which is still not very accurate, or it is?)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Profile with Nsight Compute\n",
        "!ncu --metrics lts__t_sector_hit_rate.pct,dram__bytes_read.sum,dram__bytes_write.sum,lts__t_sectors.sum python profile_matmul_membound.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Nsight Compute Results (Memory-Bound Matmul):**\n",
        "<pre style=\"background-color: #f5f5f5; color: #333; padding: 12px; border-radius: 6px; font-family: monospace;\">\n",
        "<span style=\"color: #569cd6;\">cutlass_80_simt_sgemm_128x32_8x5_nn_align1</span> (64, 1, 16)x(128, 1, 1)\n",
        "Device 0, CC 9.0\n",
        "\n",
        "Metric                       Metric Unit    Metric Value\n",
        "───────────────────────────────────────────────────────────\n",
        "dram__bytes_read.sum              Mbyte         <b>16.86</b>\n",
        "dram__bytes_write.sum             Mbyte          <b>4.38</b>\n",
        "lts__t_sector_hit_rate.pct            %         <b>55.59%</b>\n",
        "lts__t_sectors.sum               sector       <b>938,029</b>\n",
        "</pre>\n",
        "\n",
        "For this memory-bound matmul (A: 2048×2048 @ B: 2048×2), we read 16.86 MB from HBM - almost exactly the size of matrix A (16 MB). Unlike compute-bound matmuls that reread data multiple times through cache, memory-bound kernels with small outputs read each input element essentially once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Efficient Cross Entropy\n",
        "\n",
        "### The Cross Entropy Memory Problem\n",
        "\n",
        "**Notation:**\n",
        "- N = batch_size × sequence_length (number of tokens)\n",
        "- V = vocabulary size\n",
        "\n",
        "**Standard Cross Entropy Memory:**\n",
        "\n",
        "Forward pass:\n",
        "- `logits` (N, V) - input from final linear layer\n",
        "- `log_softmax(logits)` (N, V) - saved for backward\n",
        "\n",
        "Backward pass:\n",
        "- `softmax = exp(log_softmax)` (N, V) - needed to compute gradient\n",
        "- `grad_logits` (N, V) - output gradient: `softmax - one_hot(target)`\n",
        "\n",
        "**Peak memory:** 4x (N, V) tensors during backward!\n",
        "\n",
        "Example: bf16 with shape `(1, 32768, 128000)` → **8 GB per tensor**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Liger Kernel Cross Entropy\n",
        "\n",
        "Code: [Liger Kernel Cross Entropy](https://github.com/linkedin/Liger-Kernel/blob/main/src/liger_kernel/ops/cross_entropy.py).\n",
        "\n",
        "**Key Optimizations:**\n",
        "\n",
        "**1. In-place Gradient Storage**\n",
        "\n",
        "Instead of allocating a separate gradient tensor, store the gradient directly in the input logits:\n",
        "\n",
        "$$\\nabla_x L = \\text{softmax}(x) - \\text{one\\_hot}(\\text{target})$$\n",
        "\n",
        "```python\n",
        "# Forward pass computes both: loss and gradient\n",
        "# Gradient overwrites the logits in-place\n",
        "logits[i] = softmax(logits[i]) - (1 if i == target else 0)\n",
        "```\n",
        "\n",
        "**2. Online Softmax**\n",
        "\n",
        "Compute softmax statistics in a streaming fashion without materializing the full probability vector:\n",
        "\n",
        "```python\n",
        "m, d = -inf, 0.0  # running max and denominator\n",
        "for chunk in blocks(logits):\n",
        "    m_new = max(m, chunk.max())\n",
        "    d = d * exp(m - m_new) + sum(exp(chunk - m_new))\n",
        "    m = m_new\n",
        "\n",
        "lse = m + log(d)  # log-sum-exp\n",
        "loss = lse - logits[target]  # = -log(softmax[target])\n",
        "```\n",
        "\n",
        "Result: Peak memory reduced from 4x to 1x (N, V).\n",
        "\n",
        "Let's take a look at snapshots now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 ./cross_entropy/profile_vanilla_ce.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Vanilla Snapshot](images/vanilla_ce.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 ./cross_entropy/compiled_ce_snapshot.py "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Compiled Snapshot](images/compiled_ce.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 ./cross_entropy/profile_liger_ce.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Vanilla Snapshot](images/liger_ce.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fused Linear Cross Entropy (FLCE)\n",
        "\n",
        "Liger CE reduces peak memory from 4x to 1x, but still materializes the full logits tensor. FLCE avoids this by fusing the linear projection with cross-entropy computation.\n",
        "\n",
        "Code: [Liger Fused Linear Cross Entropy](https://github.com/linkedin/Liger-Kernel/blob/main/src/liger_kernel/ops/fused_linear_cross_entropy.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Liger Fused Linear Cross Entropy](images/liger_fused.avif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For further optimization of cross-entropy with large vocabularies, see Apple's Cut Cross-Entropy: [Cut Your Losses\n",
        "in Large-Vocabulary Language Models](https://arxiv.org/pdf/2411.09009v1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
